{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5bcfdc",
   "metadata": {},
   "source": [
    "Activation functions are mathematical functions used in artificial neural networks to introduce non-linearity into the network's computations. They determine the output of a neuron or a layer of neurons based on the weighted sum of inputs. Activation functions play a crucial role in enabling neural networks to learn complex patterns and relationships in data.\n",
    "\n",
    "There are several types of activation functions, each with its own characteristics and applications:\n",
    "\n",
    "1. **Sigmoid Function (Logistic)**: The sigmoid function maps input values to a range between 0 and 1, resembling an S-shaped curve. It's often used in the context of binary classification problems but can suffer from vanishing gradient problems in deep networks.\n",
    "\n",
    "2. **Hyperbolic Tangent (Tanh)**: Similar to the sigmoid, the tanh function maps input values to a range between -1 and 1, providing a slightly shifted and steeper curve. It also suffers from vanishing gradient issues.\n",
    "\n",
    "3. **Rectified Linear Unit (ReLU)**: The ReLU activation function replaces negative inputs with zero and leaves positive inputs unchanged. It's computationally efficient and has been widely adopted due to its ability to mitigate vanishing gradient problems.\n",
    "\n",
    "4. **Leaky ReLU**: Leaky ReLU is an extension of the ReLU, allowing a small gradient for negative inputs. This helps to address the \"dying ReLU\" problem where some neurons never activate during training.\n",
    "\n",
    "5. **Parametric ReLU (PReLU)**: PReLU is similar to Leaky ReLU but with a learnable parameter that determines the slope for negative inputs. This parameter can be optimized during training.\n",
    "\n",
    "6. **Exponential Linear Unit (ELU)**: ELU is another variant of ReLU that mitigates the vanishing gradient issue and includes a small negative slope for negative inputs. It also has a parameter that controls the saturation point for extremely negative inputs.\n",
    "\n",
    "7. **Scaled Exponential Linear Unit (SELU)**: SELU is a self-normalizing activation function that aims to maintain a mean and variance of activations close to 0 and 1, respectively. It's designed to work well in deep networks without requiring extensive parameter tuning.\n",
    "\n",
    "8. **Softmax**: Softmax is used primarily in the output layer for multiclass classification problems. It converts a vector of raw scores into a probability distribution over multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a7cf2",
   "metadata": {},
   "source": [
    "Certainly, here are the mathematical expressions for the activation functions mentioned earlier:\n",
    "\n",
    "1. **Sigmoid Function (Logistic)**:\\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "\n",
    "2. **Hyperbolic Tangent (Tanh)**:\n",
    "   \\( \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
    "\n",
    "3. **Rectified Linear Unit (ReLU)**:\n",
    "   \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
    "\n",
    "4. **Leaky ReLU**:\n",
    "   \\( \\text{Leaky ReLU}(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ \\alpha x & \\text{if } x < 0 \\end{cases} \\)\n",
    "   where \\( \\alpha \\) is a small positive constant (typically a small fraction like 0.01).\n",
    "\n",
    "5. **Parametric ReLU (PReLU)**:\n",
    "   \\( \\text{PReLU}(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ \\alpha x & \\text{if } x < 0 \\end{cases} \\)\n",
    "   where \\( \\alpha \\) is a learnable parameter.\n",
    "\n",
    "6. **Exponential Linear Unit (ELU)**:\n",
    "   \\( \\text{ELU}(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ \\alpha (e^x - 1) & \\text{if } x < 0 \\end{cases} \\)\n",
    "   where \\( \\alpha \\) is a positive constant, typically around 1.0.\n",
    "\n",
    "7. **Scaled Exponential Linear Unit (SELU)**:\n",
    "   \\( \\text{SELU}(x) = \\lambda \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha e^x - \\alpha & \\text{if } x \\leq 0 \\end{cases} \\)\n",
    "   where \\( \\alpha \\) and \\( \\lambda \\) are hyperparameters that help maintain activations near mean 0 and variance 1.\n",
    "\n",
    "8. **Softmax** (for \\( i \\)th element of the input vector \\( \\mathbf{x} \\)):\n",
    "   \\( \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^N e^{x_j}} \\)\n",
    "   where \\( N \\) is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc6790",
   "metadata": {},
   "source": [
    "### 1. Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cedd8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a sigmoid function\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80022082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of sigmoid at i = -100 is: 3.72e-44\n",
      "The range of sigmoid at i = -75 is: 2.68e-33\n",
      "The range of sigmoid at i = -50 is: 1.93e-22\n",
      "The range of sigmoid at i = -25 is: 1.39e-11\n",
      "The range of sigmoid at i = 0 is: 5.00e-01\n",
      "The range of sigmoid at i = 25 is: 1.00e+00\n",
      "The range of sigmoid at i = 50 is: 1.00e+00\n",
      "The range of sigmoid at i = 75 is: 1.00e+00\n",
      "The range of sigmoid at i = 100 is: 1.00e+00\n"
     ]
    }
   ],
   "source": [
    "## Testing different values for sigmoid function\n",
    "for i in range(-100, 101, 25):\n",
    "    result = sigmoid(i)\n",
    "    formatted_result = \"{:.2e}\".format(result)\n",
    "    print(f\"The range of sigmoid at i = {i} is: {formatted_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0ea01",
   "metadata": {},
   "source": [
    "### 2. Tanh Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8d51b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1865f600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of tanh at i = -100 is: -1.00e+00\n",
      "The range of tanh at i = -75 is: -1.00e+00\n",
      "The range of tanh at i = -50 is: -1.00e+00\n",
      "The range of tanh at i = -25 is: -1.00e+00\n",
      "The range of tanh at i = 0 is: 0.00e+00\n",
      "The range of tanh at i = 25 is: 1.00e+00\n",
      "The range of tanh at i = 50 is: 1.00e+00\n",
      "The range of tanh at i = 75 is: 1.00e+00\n",
      "The range of tanh at i = 100 is: 1.00e+00\n"
     ]
    }
   ],
   "source": [
    "## Testing different values for tanh function\n",
    "for i in range(-100, 101, 25):\n",
    "    result = tanh(i)\n",
    "    formatted_result = \"{:.2e}\".format(result)\n",
    "    print(f\"The range of tanh at i = {i} is: {formatted_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434369e",
   "metadata": {},
   "source": [
    "### 3. Relu Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d338c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a2ef1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of relu at i = -100 is: 0.00e+00\n",
      "The range of relu at i = -75 is: 0.00e+00\n",
      "The range of relu at i = -50 is: 0.00e+00\n",
      "The range of relu at i = -25 is: 0.00e+00\n",
      "The range of relu at i = 0 is: 0.00e+00\n",
      "The range of relu at i = 25 is: 2.50e+01\n",
      "The range of relu at i = 50 is: 5.00e+01\n",
      "The range of relu at i = 75 is: 7.50e+01\n",
      "The range of relu at i = 100 is: 1.00e+02\n"
     ]
    }
   ],
   "source": [
    "## Testing different values for relu function\n",
    "for i in range(-100, 101, 25):\n",
    "    result = relu(i)\n",
    "    formatted_result = \"{:.2e}\".format(result)\n",
    "    print(f\"The range of relu at i = {i} is: {formatted_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe7efb",
   "metadata": {},
   "source": [
    "### 4. Leaky Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3b6f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return max(0.1*x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6503ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of leaky relu at i = -100 is: -1.00e+01\n",
      "The range of leaky relu at i = -75 is: -7.50e+00\n",
      "The range of leaky relu at i = -50 is: -5.00e+00\n",
      "The range of leaky relu at i = -25 is: -2.50e+00\n",
      "The range of leaky relu at i = 0 is: 0.00e+00\n",
      "The range of leaky relu at i = 25 is: 2.50e+01\n",
      "The range of leaky relu at i = 50 is: 5.00e+01\n",
      "The range of leaky relu at i = 75 is: 7.50e+01\n",
      "The range of leaky relu at i = 100 is: 1.00e+02\n"
     ]
    }
   ],
   "source": [
    "## Testing different values for leaky relu function\n",
    "for i in range(-100, 101, 25):\n",
    "    result = leaky_relu(i)\n",
    "    formatted_result = \"{:.2e}\".format(result)\n",
    "    print(f\"The range of leaky relu at i = {i} is: {formatted_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96708e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
